{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro_1",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "# Assignment 2: Markov Decision Processes\n",
    "\n",
    "\n",
    "## Homework Instructions\n",
    "All your answers should be written in this notebook.  You shouldn't need to write or modify any other files.\n",
    "Look for four instances of \"YOUR CODE HERE\"--those are the only parts of the code you need to write. To grade your homework, we will check whether the printouts immediately following your code match up with the results we got. The portions used for grading are highlighted in yellow. (However, note that the yellow highlighting does not show up when github renders this file.)\n",
    "\n",
    "To submit your homework, send an email to <berkeleydeeprlcourse@gmail.com> with the subject line \"Deep RL Assignment 2\" and two attachments:\n",
    "1. This `ipynb` file\n",
    "2. A pdf version of this file (To make the pdf, do `File - Print Preview`)\n",
    "\n",
    "The homework is due Febrary 22nd, 11:59 pm.\n",
    "\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro_2",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This assignment will review the two classic methods for solving Markov Decision Processes (MDPs) with finite state and action spaces.\n",
    "We will implement value iteration (VI) and policy iteration (PI) for a finite MDP, both of which find the optimal policy in a finite number of iterations.\n",
    "\n",
    "The experiments here will use the Frozen Lake environment, a simple gridworld MDP that is taken from `gym` and slightly modified for this assignment. In this MDP, the agent must navigate from the start state to the goal state on a 4x4 grid, with stochastic transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Winter is here. You and your friends were tossing around a frisbee at the park\n",
      "    when you made a wild throw that left the frisbee out in the middle of the lake.\n",
      "    The water is mostly frozen, but there are a few holes where the ice has melted.\n",
      "    If you step into one of those holes, you'll fall into the freezing water.\n",
      "    At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
      "    you navigate across the lake and retrieve the disc.\n",
      "    However, the ice is slippery, so you won't always move in the direction you intend.\n",
      "    The surface is described using a grid like the following\n",
      "\n",
      "        SFFF\n",
      "        FHFH\n",
      "        FFFH\n",
      "        HFFG\n",
      "\n",
      "    S : starting point, safe\n",
      "    F : frozen surface, safe\n",
      "    H : hole, fall to your doom\n",
      "    G : goal, where the frisbee is located\n",
      "\n",
      "    The episode ends when you reach the goal or fall in a hole.\n",
      "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from frozen_lake import FrozenLakeEnv\n",
    "env = FrozenLakeEnv()\n",
    "print(env.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what a random episode looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Some basic imports and setup\n",
    "import numpy as np, numpy.random as nr, gym\n",
    "np.set_printoptions(precision=3)\n",
    "def begin_grading(): print(\"\\x1b[43m\")\n",
    "def end_grading(): print(\"\\x1b[0m\")\n",
    "\n",
    "# Seed RNGs so you get the same printouts as me\n",
    "env.seed(0); from gym.spaces import prng; prng.seed(10)\n",
    "# Generate the episode\n",
    "env.reset()\n",
    "for t in range(100):\n",
    "    env.render()\n",
    "    a = env.action_space.sample()\n",
    "    ob, rew, done, _ = env.step(a)\n",
    "    if done:\n",
    "        break\n",
    "assert done\n",
    "env.render();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the episodeÂ above, the agent falls into a hole after two timesteps. Also note the stochasticity--on the first step, the DOWN action is selected, but the agent moves to the right.\n",
    "\n",
    "We extract the relevant information from the gym Env into the MDP class below.\n",
    "The `env` object won't be used any further, we'll just use the `mdp` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdp.P is a two-level dict where the first key is the state and the second key is the action.\n",
      "The 2D grid cells are associated with indices [0, 1, 2, ..., 15] from left to right and top to down, as in\n",
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]]\n",
      "mdp.P[state][action] is a list of tuples (probability, nextstate, reward).\n",
      "\n",
      "For example, state 0 is the initial state, and the transition information for s=0, a=0 is \n",
      "P[0][0] = [(0.1, 0, 0.0), (0.8, 0, 0.0), (0.1, 4, 0.0)] \n",
      "\n",
      "As another example, state 5 corresponds to a hole in the ice, which transitions to itself with probability 1 and reward 0.\n",
      "P[5][0] = [(1.0, 5, 0)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class MDP(object):\n",
    "    def __init__(self, P, nS, nA, desc=None):\n",
    "        self.P = P # state transition and reward probabilities, explained below\n",
    "        self.nS = nS # number of states\n",
    "        self.nA = nA # number of actions\n",
    "        self.desc = desc # 2D array specifying what each grid cell means (used for plotting)\n",
    "mdp = MDP( {s : {a : [tup[:3] for tup in tups] for (a, tups) in a2d.items()} for (s, a2d) in env.P.items()}, env.nS, env.nA, env.desc)\n",
    "\n",
    "\n",
    "print(\"mdp.P is a two-level dict where the first key is the state and the second key is the action.\")\n",
    "print(\"The 2D grid cells are associated with indices [0, 1, 2, ..., 15] from left to right and top to down, as in\")\n",
    "print(np.arange(16).reshape(4,4))\n",
    "print(\"mdp.P[state][action] is a list of tuples (probability, nextstate, reward).\\n\")\n",
    "print(\"For example, state 0 is the initial state, and the transition information for s=0, a=0 is \\nP[0][0] =\", mdp.P[0][0], \"\\n\")\n",
    "print(\"As another example, state 5 corresponds to a hole in the ice, which transitions to itself with probability 1 and reward 0.\")\n",
    "print(\"P[5][0] =\", mdp.P[5][0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "4",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "## Part 1: Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "### Problem 1: implement value iteration\n",
    "In this problem, you'll implement value iteration, which has the following pseudocode:\n",
    "\n",
    "---\n",
    "Initialize $V^{(0)}(s)=0$, for all $s$\n",
    "\n",
    "For $i=0, 1, 2, \\dots$\n",
    "- $V^{(i+1)}(s) = \\max_a \\sum_{s'} P(s,a,s') [ R(s,a,s') + \\gamma V^{(i)}(s')]$, for all $s$\n",
    "\n",
    "---\n",
    "\n",
    "We additionally define the sequence of greedy policies $\\pi^{(0)}, \\pi^{(1)}, \\dots, \\pi^{(n-1)}$, where\n",
    "$$\\pi^{(i)}(s) = \\arg \\max_a \\sum_{s'} P(s,a,s') [ R(s,a,s') + \\gamma V^{(i)}(s')]$$\n",
    "\n",
    "Your code will return two lists: $[V^{(0)}, V^{(1)}, \\dots, V^{(n)}]$ and $[\\pi^{(0)}, \\pi^{(1)}, \\dots, \\pi^{(n-1)}]$\n",
    "\n",
    "To ensure that you get the same policies as the reference solution, choose the lower-index action to break ties in $\\arg \\max_a$. This is done automatically by np.argmax. This will only affect the \"# chg actions\" printout below--it won't affect the values computed.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Warning: make a copy of your value function each iteration and use that copy for the update--don't update your value function in place. \n",
    "Updating in-place is also a valid algorithm, sometimes called Gauss-Seidel value iteration or asynchronous value iteration, but it will cause you to get different results than me.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q function:\n",
      "\n",
      "[[ 0.07   0.091  0.053  0.073]\n",
      " [ 0.057  0.001  0.04   0.054]\n",
      " [ 0.024  0.088  0.006  0.006]\n",
      " [ 0.008  0.     0.     0.   ]\n",
      " [ 0.092  0.129  0.012  0.054]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [ 0.     0.173  0.049  0.013]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [ 0.146  0.059  0.277  0.097]\n",
      " [ 0.135  0.355  0.309  0.087]\n",
      " [ 0.159  0.578  0.021  0.055]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [ 0.021  0.276  0.521  0.209]\n",
      " [ 0.225  0.642  0.935  0.371]\n",
      " [ 0.     0.     0.     0.   ]]\n",
      "\n",
      "Greedy algorithm:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANQAAADGCAYAAACqwK6IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd67f767160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import datetime\n",
    "\n",
    "def sarsa_lambda(env, gamma, delta, rate, epsilon, nIt, render):\n",
    "    \"\"\"Salsa(lambda) algorithm\n",
    "        \n",
    "    Args:\n",
    "        env: environment\n",
    "        gamma: decay of reward\n",
    "        delta: the lambda parameter for Salsa(lambda) algorithm\n",
    "        rate: learning rate\n",
    "        nIt: number of iterations\n",
    "        render: boolean which determines if render the state or not        \n",
    "    \"\"\"\n",
    "    random.seed(datetime.datetime.now().timestamp())\n",
    "    \n",
    "    q = np.array([0] * env.nS * env.nA, dtype = float).reshape(env.nS, env.nA)\n",
    "    for i in range(nIt):\n",
    "        trace = np.zeros_like(q)\n",
    "        \n",
    "        obs_prev = None\n",
    "        act_prev = None\n",
    "        \n",
    "        obs = None\n",
    "        done = False\n",
    "        totalr = 0.\n",
    "        # Need to reorganize the code a little bit as Sarsa(lambda) needs an extra action sampling\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "                \n",
    "            if obs is None: \n",
    "                obs = env.reset()\n",
    "            else:\n",
    "                assert act is not None\n",
    "                obs, r, done, _ = env.step(act)\n",
    "                totalr += r\n",
    "            \n",
    "            p = np.random.uniform(0., 1.)\n",
    "            if p > epsilon:\n",
    "                act = np.argmax(q[obs])\n",
    "            else:\n",
    "                act = np.random.randint(env.nA)\n",
    "            # Sarsa(delta)\n",
    "            # R and S are ready. Waiting for A. \n",
    "            if obs_prev is not None:\n",
    "                trace *= delta * gamma\n",
    "                trace[obs_prev][act_prev] += 1\n",
    "                q += rate * trace * (r + gamma * q[obs][act] - q[obs_prev][act_prev])\n",
    "            obs_prev = obs\n",
    "            act_prev = act\n",
    "            \n",
    "        if render:\n",
    "            env.render()\n",
    "    \n",
    "    return q\n",
    "\n",
    "gamma = 0.9  # decay of reward\n",
    "delta = 0.5  # decay of eligibility trace\n",
    "rate = 0.1  # the learning rate, or alpha in the book\n",
    "nIt = 1000\n",
    "epsilon = 0.5  # epsilon greedy\n",
    "q = sarsa_lambda(env, gamma, delta, rate, epsilon, nIt, False)\n",
    "print(\"Q function:\\n\")\n",
    "print(q)\n",
    "\n",
    "print()\n",
    "print(\"Greedy algorithm:\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def policy_matrix(q):\n",
    "    indices = np.argmax(q, axis = 1)\n",
    "    indices[np.max(q, axis = 1) == 0] = 4\n",
    "    to_direction = np.vectorize(lambda x: ['L', 'D', 'R', 'U', ''][x])\n",
    "    return to_direction(indices.reshape(4, 4))\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "# imshow makes top left the origin\n",
    "plt.imshow(np.array([0] * 16).reshape(4,4), cmap='gray', interpolation='none', clim=(0,1))\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(4)-.5)\n",
    "ax.set_yticks(np.arange(4)-.5)\n",
    "directions = policy_matrix(q)\n",
    "for y in range(4):\n",
    "    for x in range(4):\n",
    "        plt.text(x, y, str(env.desc[y,x].item().decode()) + ',' + directions[y, x],\n",
    "                    color='g', size=12,  verticalalignment='center',\n",
    "                 horizontalalignment='center', fontweight='bold')\n",
    "plt.grid(color='b', lw=2, ls='-')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
